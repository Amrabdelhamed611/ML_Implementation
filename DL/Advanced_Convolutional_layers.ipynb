{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d8d261",
   "metadata": {},
   "source": [
    "# <font style=\"color:#0015FF\">Advanced Convolutional Neural Networks</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4bb8d",
   "metadata": {},
   "source": [
    "by: **Amr Abdelhamed**\n",
    "\n",
    "**[Linkedin profile](https://www.linkedin.com/in/amrabdelhamed69/)\n",
    "[Github repo](https://github.com/Amrabdelhamed611/ML_Implementation/tree/main/DL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb86845",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Residual-Block\" data-toc-modified-id=\"Residual-Block-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Residual Block</a></span></li><li><span><a href=\"#Bottleneck-Residual-Block\" data-toc-modified-id=\"Bottleneck-Residual-Block-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bottleneck Residual Block</a></span></li><li><span><a href=\"#Linear-BottleNecks\" data-toc-modified-id=\"Linear-BottleNecks-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Linear BottleNecks</a></span></li><li><span><a href=\"#Inverted-Residual-Block\" data-toc-modified-id=\"Inverted-Residual-Block-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Inverted Residual Block</a></span></li><li><span><a href=\"#Depth-Wise-Separable-Convolution\" data-toc-modified-id=\"Depth-Wise-Separable-Convolution-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Depth-Wise Separable Convolution</a></span></li><li><span><a href=\"#Squeeze-and-Excitation-Block\" data-toc-modified-id=\"Squeeze-and-Excitation-Block-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Squeeze and Excitation Block</a></span></li><li><span><a href=\"#MBConv\" data-toc-modified-id=\"MBConv-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>MBConv</a></span></li><li><span><a href=\"#Fused-Inverted-Residual-(Fused-MBConv)-:\" data-toc-modified-id=\"Fused-Inverted-Residual-(Fused-MBConv)-:-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Fused Inverted Residual (Fused MBConv) :</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d440e600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:54.933822Z",
     "start_time": "2022-04-25T16:09:51.807780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch ,torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3260c54e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:54.978702Z",
     "start_time": "2022-04-25T16:09:54.935817Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 4, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBnAct(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to add block of 3 layers (conv  layer ,batch-normalization , activation function).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size= 3,\n",
    "                 conv_kwargs={},\n",
    "                 Bn = nn.BatchNorm2d,\n",
    "                 actvtion = nn.SiLU,\n",
    "                 actvtion_kwargs={}):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            kernel_size : int (default is 3)\n",
    "                kernal size for conv layer.\n",
    "            conv_kwargs : dictionary (default empty dictionary )\n",
    "                pass customize keyword arguments to conv2d function.\n",
    "            Bn : nn.module.batchnorm (default is nn.BatchNorm2d)\n",
    "                Batch normlizetion layer to Add.\n",
    "            actvtion : nn.modules.activation (default is SilU)\n",
    "                Activation function.\n",
    "            actvtion_kwargs : dictionary (default empty dictionary )\n",
    "                pass customize keyword arguments to Actvtion function.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ConvBnAct, self).__init__()\n",
    "    \n",
    "        self.Conv_BN_Act = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,bias=False,kernel_size= kernel_size,**conv_kwargs),\n",
    "            Bn(out_channels),\n",
    "            actvtion(**actvtion_kwargs))\n",
    "            # bias set to  false as we use BN\n",
    "    def forward(self, x):\n",
    "        return self.Conv_BN_Act(x)\n",
    "#--------------------------------------------------------------------\n",
    "#C_kwargs={'stride':1,'kernel_size':1,'padding' :0,'groups': 1,'padding_mode':'same'}\n",
    "#A_kwargs={'inplace':True}\n",
    "Conv1X1BnAct = partial(ConvBnAct,kernel_size= 1)\n",
    "Conv3X3BnAct  = partial(ConvBnAct,kernel_size= 3)\n",
    "\n",
    "x = torch.randn((1, 16, 6, 6))\n",
    "Conv3X3BnAct(16, 16)(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b92997",
   "metadata": {},
   "source": [
    "## Residual Block\n",
    "use direct connection to skip some layers(called skip connection) , in a way the connection passes input $x$ deeper in the network without apply any transformation on it.\n",
    "\n",
    "the output of the layer is without skip connection: $H(x)= F(wx+b) $ where $F$ an activation function.\n",
    "skip connection is element-wise addition of the input of the Residual Block $x$ and the output $H(X)$.\n",
    " \n",
    "and we must consider to check o the shape of residuals $x$ and outputs $H(x)$:\n",
    "* if they shapes are equales we carry on the addition operation.\n",
    "* if they not equales we can use conv layer on the short cut to reduce the shape of residuals $x$ or pad $H(x)$ to increase the shape of $H(x)$.\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_1.jpg\" alt=\"Residual Block\" style=\"width:300px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d12f154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.008622Z",
     "start_time": "2022-04-25T16:09:54.979699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 1, 3, 3])\n",
      "output shape of conv block: torch.Size([1, 1, 3, 3])\n",
      "output shape of res bloc: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a residual block by adding skip for given module.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, block,shortcutblock = None):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            block : nn.module \n",
    "                block to add skip conection to connect bloc input to its output.\n",
    "            shortcutblock : nn.module\n",
    "                shortcutblock a nn layer  (conv,pool)  to match shape of resduals and outputs\n",
    "\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = block\n",
    "        self.shortcut= shortcutblock\n",
    "    def forward(self, x):\n",
    "        xres=x\n",
    "        x = self.block(x)\n",
    "        if x.shape ==xres.shape:\n",
    "            x = x+xres\n",
    "        else:\n",
    "            assert isinstance(self.shortcut,nn.Module),'shortcutblock must be conv layer to match shape of resduals and outputs'\n",
    "            x= self.shortcut(xres)+x\n",
    "        return x\n",
    "    \n",
    "x = torch.rand((1,1,3,3))\n",
    "nnconv = nn.Conv2d(in_channels=1,out_channels=1,kernel_size= 1,stride= 1)\n",
    "resconv= ResBlock(nnconv)\n",
    "\n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape of conv block: {nnconv(x).shape}')\n",
    "print(f'output shape of res bloc: {resconv(x).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50986c9",
   "metadata": {},
   "source": [
    "##  Bottleneck Residual Block\n",
    "bottleneck layer has fewer neurons than the layer below or above it.such a layer encourages the network to compress feature representations to best fit in the available space.\n",
    "\n",
    "A Bottleneck Residual Block takes an input of shape `HxWxC`, then reduces it to `HxWxC/r` using `1x1` conv, then applies a `3x3` conv and finally scale up the output to the same feature dimension as the input,`HxWxC` using again a 1x1 conv , and use skip connection to add the bottleneck output to the . \n",
    "\n",
    "A Bottleneck Residual Block combine both Bottleneck Block and Residual Block by adding skip connection to skip the bottleneck block.\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_2.jpg\" alt=\"Residual Block\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de274fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.038543Z",
     "start_time": "2022-04-25T16:09:55.010617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 5, 4, 4])\n",
      "output shape: torch.Size([1, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a Bottleneck Block.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, compersion_ratio = 4):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            compersion_ratio : int (default is 4)\n",
    "                feature compersion ratio.\n",
    "        \"\"\"\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        reduced_channels = out_channels// compersion_ratio\n",
    "        C_kwargs={'stride':1,'padding' :'same'}\n",
    "        A_kwargs={'inplace':True}\n",
    "        block = nn.Sequential(\n",
    "                        Conv1X1BnAct(in_channels, reduced_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv3X3BnAct(reduced_channels, reduced_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv1X1BnAct(reduced_channels, out_channels ,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs))\n",
    "        shortcut = Conv1X1BnAct(in_channels, out_channels,\n",
    "                                conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs)  if in_channels != out_channels else None\n",
    "        self.BottleNeck = ResBlock(block ,shortcut)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.BottleNeck(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 5, 4,4))  \n",
    "BBn =BottleneckBlock(5,4)    \n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {BBn(x).shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be445fcf",
   "metadata": {},
   "source": [
    "## Linear BottleNecks\n",
    "Linear BottleNecks were introduced in MobileNetV2. A Linear BottleNeck Block is a BottleNeck Block without the last activation. the authors discussed how loss in performance due to dying rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a874d72d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.053502Z",
     "start_time": "2022-04-25T16:09:55.040538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 4, 4, 4])\n",
      "output shape: torch.Size([1, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class LinearBottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a Linear Bottleneck Block.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, compersion_ratio = 4):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            compersion_ratio : int (default is 4)\n",
    "                feature compersion ratio.\n",
    "        \"\"\"\n",
    "        super(LinearBottleneckBlock, self).__init__()\n",
    "        reduced_channels = out_channels// compersion_ratio\n",
    "        C_kwargs={'stride':1,'padding' :'same'}\n",
    "        A_kwargs={'inplace':True}\n",
    "        \n",
    "        block = nn.Sequential(\n",
    "                        Conv1X1BnAct(in_channels, reduced_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv3X3BnAct(reduced_channels, reduced_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv1X1BnAct(reduced_channels, out_channels, actvtion=nn.Identity ,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs ))\n",
    "        shortcut = Conv1X1BnAct(in_channels, out_channels,\n",
    "                                    conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs)  if in_channels != out_channels else None\n",
    "        self.BottleNeck = ResBlock(block ,shortcut)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x= self.BottleNeck(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 4, 4,4))  \n",
    "BBn =LinearBottleneckBlock(4,4)    \n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {BBn(x).shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d7061",
   "metadata": {},
   "source": [
    "## Inverted Residual Block\n",
    "think of inverted residual as Bottleneck Residual Block but expand the features maps instead of reduce them.\n",
    "\n",
    "Inverted Residual Block: takes an input of shape `HxWxC`, then expand it to `HxWxC*e` using `1x1` conv, then applies a `3x3` conv and finally scale up the output to the same feature dimension as the input `HxWxC` using again a `1x1` conv , and use skip connection to add the bottleneck output to the .\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_3.jpg\" alt=\"Residual Block\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cc29e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.068462Z",
     "start_time": "2022-04-25T16:09:55.054500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 4, 3, 3])\n",
      "output shape: torch.Size([1, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a Inverted Residual Block.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expansion_ratio = 4):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            expansion_ratio : int (default is 4)\n",
    "                feature Expansion ratio.\n",
    "        \"\"\"\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        expanded_channels = int(in_channels*expansion_ratio)\n",
    "        C_kwargs={'stride':1,'padding' :'same'}\n",
    "        A_kwargs={'inplace':True}\n",
    "        block = nn.Sequential(\n",
    "                        Conv1X1BnAct(in_channels, expanded_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv3X3BnAct(expanded_channels, expanded_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv1X1BnAct(expanded_channels, out_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs))\n",
    "        shortcut = Conv1X1BnAct(in_channels, out_channels,\n",
    "                                conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs)  if in_channels != out_channels else None\n",
    "        self.Block = ResBlock(block ,shortcut)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.Block(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 4, 3,3))  \n",
    "\n",
    "IR =InvertedResidualBlock(4,5)    \n",
    "\n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {IR(x).shape}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d938c",
   "metadata": {},
   "source": [
    "## Depth-Wise Separable Convolution\n",
    "\n",
    "The idea is spatial dimension `HxW` of a filter can be separated from the depth `C` of the filter by apply filter on the the spatial dimension and output will has the same depth as the input the filter will only apply on the `Hxw`.\n",
    "\n",
    "Depth-Wise Separable Convolutions for `HxWxC`applies single `3x3xC` filter to each input's channels, then a N `1x1xc` conv to all the channels so the output `HxWxN` .hence, spatial dimension `HxW` may change depend on the stride and padding using.\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_4.jpg\" alt=\"Residual Block\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "were `3x3` filter  called depth wise and `1x1` called point wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc51a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.113872Z",
     "start_time": "2022-04-25T16:09:55.070458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Depth-Wise Separable Convolution output: [1, 3, 5, 5] \n",
      "shape of tradtional Convolution output: [1, 3, 5, 5] \n",
      "parameter count of Depth-Wise Separable Convolution : 48 \n",
      "parameter count of tradtional Convolution :  84 \n",
      "parameter count of Depth-Wise Separable Convolution with more channels : 2528\n",
      "parameter count of traditional Convolution with more channels :  18496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DWSConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a  Depth-Wise Separable Convolution.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "        \"\"\"\n",
    "        super(DWSConv, self).__init__()\n",
    "        C1_kwargs={'stride':1,'padding' :'same','groups': in_channels}\n",
    "        C2_kwargs={'stride':1,'padding' :'same'}\n",
    "        A_kwargs={'inplace':True}\n",
    "        self.Block = nn.Sequential(\n",
    "                        Conv3X3BnAct(in_channels,in_channels,\n",
    "                                     conv_kwargs=C1_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv1X1BnAct(in_channels, out_channels,\n",
    "                                     conv_kwargs=C2_kwargs,actvtion_kwargs=A_kwargs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= self.Block(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 3, 5,5))  \n",
    "DWS =DWSConv(3,3)   \n",
    "tconv =nn.Conv2d(3,3,kernel_size=3,stride=1,padding=1)\n",
    "#print(x.view(3,5,5))\n",
    "#print(DWS(x))\n",
    "\n",
    "print(f'''shape of Depth-Wise Separable Convolution output: {list(DWS(x).shape)} \n",
    "shape of tradtional Convolution output: {list(tconv (x).shape)} \n",
    "parameter count of Depth-Wise Separable Convolution : {sum(p.numel() for p in DWS.parameters() if p.requires_grad)} \n",
    "parameter count of tradtional Convolution :  {sum(p.numel() for p in tconv.parameters() if p.requires_grad)} \n",
    "parameter count of Depth-Wise Separable Convolution with more channels : {sum(p.numel() for p in DWSConv(32,64).parameters() if p.requires_grad)}\n",
    "parameter count of traditional Convolution with more channels :  {sum(p.numel() for p in nn.Conv2d(32, 64, kernel_size=3).parameters() if p.requires_grad)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c897c5",
   "metadata": {},
   "source": [
    "there is difference between traditional Convolutions Depth-Wise Separable Convolutions in parameter count specially when has more bigger number of channels so that makes Depth-Wise Separable Convolutions more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a779170",
   "metadata": {},
   "source": [
    "## Squeeze and Excitation Block\n",
    "squeeze excitation makes the neural nets able to map the channels dependency along with access to global information (means use information from the whole batch not only the sample). \n",
    " \n",
    "**Squeeze :**\n",
    "used to extract the global information from each channel of the feature map `BxCxHxW` (wher B is batch size).\n",
    "\n",
    "the convolution is a local operation on parts of feature map .So, it is beneficial to get a global understanding of the feature map by use global pooling to reduce the spatial dimensions from `BxCxHxW` to `BxCx1x1`,by using Max Pooling either Average Pooling (the paper authors found Average Pooling has lower error)\n",
    "\n",
    "**Excitation :**\n",
    "The feature map reduced to `BxCx1x1`,excitation operation use a fully connected Layer with a bottleneck structure to generate the weights for each channel of the feature map adaptively.\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_5.jpg\" alt=\"Residual Block\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9745ea7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.128832Z",
     "start_time": "2022-04-25T16:09:55.114869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 3, 5, 5])\n",
      "output shape: torch.Size([1, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class SE(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement a Squeeze and Excitation Block.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels, squeeze_ratio=8):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            Squeeze_ratio : int (default is 8)\n",
    "                feature reduction ratio.\n",
    "        \"\"\"\n",
    "        super(SE, self).__init__()\n",
    "        squeezed_channels = channels//squeeze_ratio\n",
    "        self.SEBlock =  nn.Sequential(\n",
    "                                nn.AdaptiveAvgPool2d(1),\n",
    "                                nn.Conv2d(channels,squeezed_channels,kernel_size=1),\n",
    "                                nn.SiLU(inplace=True),\n",
    "                                nn.Conv2d(squeezed_channels, channels,kernel_size =1),\n",
    "                                nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.SEBlock(x)\n",
    "    \n",
    "x = torch.randn((1, 3, 5,5))  \n",
    "se=SE(3,3)   \n",
    "\n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {se(x).shape}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee2877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T14:50:58.588691Z",
     "start_time": "2021-12-21T14:50:58.576078Z"
    }
   },
   "source": [
    "## MBConv \n",
    "a key component in Efficient net.\n",
    "it is slightly different then Inverted Residual :\n",
    "* normalization is applied to both depth and point convolution. \n",
    "* non-linearity only in the depth convolution.\n",
    "\n",
    "<img src=\"./attachments/cnn_layers_image_6.jpg\" alt=\"Residual Block\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95ce081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.158814Z",
     "start_time": "2022-04-25T16:09:55.129830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 4, 3, 3])\n",
      "output shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class MBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement Inverted Residual Block used in mobilenetv2.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expansion_ratio = 4,squeeze_ratio=4):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            expansion_ratio : int (default is 4)\n",
    "                feature reduction ratio.\n",
    "            Squeeze_ratio : int (default is 4)\n",
    "                feature reduction ratio.\n",
    "        \"\"\"\n",
    "        super(MBConv, self).__init__()\n",
    "        expanded_channels = int(in_channels*expansion_ratio)\n",
    "        C_kwargs={'stride':1,'padding' :'same'}\n",
    "        C1_kwargs={'stride':1,'padding' :'same','groups': expanded_channels}\n",
    "        A_kwargs={'inplace':True}\n",
    "        block = nn.Sequential(\n",
    "                        Conv1X1BnAct(in_channels, expanded_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        Conv3X3BnAct(expanded_channels, expanded_channels,\n",
    "                                     conv_kwargs=C1_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        SE(expanded_channels,squeeze_ratio),\n",
    "                        Conv1X1BnAct(expanded_channels, out_channels,actvtion=nn.Identity,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs))\n",
    "       \n",
    "        self.Block = nn.Sequential( ResBlock(block) ,nn.SiLU(inplace=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.Block(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 4, 3,3))  \n",
    "\n",
    "mbconv =MBConv(4,4)    \n",
    "#print(x.view(4,3,3))\n",
    "#print(mbconv(x).view(4,3,3))\n",
    "\n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {mbconv(x).shape}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428806c",
   "metadata": {},
   "source": [
    "## Fused Inverted Residual (Fused MBConv) :\n",
    "\n",
    "Fused Inverted Residuals were introduced in EfficientNetV2 Smaller Models and Faster Training to make MBConv faster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b39bb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T16:09:55.188774Z",
     "start_time": "2022-04-25T16:09:55.160787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 4, 3, 3])\n",
      "output shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class FusedMBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to implement Inverted Residual Block used in EfficientNetV2.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expansion_ratio = 4,squeeze_ratio=4):\n",
    "        \"\"\"\n",
    "        Constructs the there layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            in_channels : int \n",
    "                block input channels.\n",
    "            out_channels : int\n",
    "                block output channels.\n",
    "            expansion_ratio : int (default is 4)\n",
    "                feature reduction ratio.\n",
    "            Squeeze_ratio : int (default is 4)\n",
    "                feature reduction ratio.\n",
    "        \"\"\"\n",
    "        super(FusedMBConv, self).__init__()\n",
    "        expanded_channels = int(in_channels*expansion_ratio)\n",
    "        C_kwargs={'stride':1,'padding' :'same'}\n",
    "        A_kwargs={'inplace':True}\n",
    "        block = nn.Sequential(\n",
    "                        Conv3X3BnAct(in_channels, expanded_channels,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),\n",
    "                        SE(expanded_channels,squeeze_ratio),\n",
    "                        Conv1X1BnAct(expanded_channels, out_channels,actvtion=nn.Identity,\n",
    "                                     conv_kwargs=C_kwargs,actvtion_kwargs=A_kwargs),)\n",
    "       \n",
    "        self.Block = nn.Sequential( ResBlock(block) ,nn.SiLU(inplace=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.Block(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn((1, 4, 3,3))  \n",
    "\n",
    "fusedmbconv =FusedMBConv(4,4)    \n",
    "#print(x.view(4,3,3))\n",
    "#print(fusedmbconv(x).view(4,3,3))\n",
    "print(f'input shape: {x.shape}')\n",
    "print(f'output shape: {fusedmbconv(x).shape}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139f0df",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now we have the building blocks of many **convolutional neural network architectures** like `Res-Nets`, `SE-Net`, `Mobile-Nets`, `Efficient-Nets` so we can read , understand , implement various papers which uses the building blocks we Built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542623a2",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* [Brief introduction of mobilenetv1 / V2 / V3 lightweight network ](https://developpaper.com/brief-introduction-of-mobilenetv1-v2-v3-lightweight-network/)\n",
    "* [Inverted Residual Block paper](https://paperswithcode.com/method/inverted-residual-block)\n",
    "* [Squeeze and Excitation Networks Explained ](https://amaarora.github.io/2020/07/24/SeNet.html)\n",
    "* [Squeeze and Excitation Networks By Nikhil Tomar](https://medium.com/analytics-vidhya/squeeze-and-excitation-networks-idiot-developer-17de2fd02596?utm_source=pocket_mylist)\n",
    "* [EfficientNetV2 paper ](https://paperswithcode.com/method/inverted-residual-block)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "195.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
